![Version: 1.0.0](https://img.shields.io/badge/Version-1.0.0-blue.svg)
![Status: Draft](https://img.shields.io/badge/Status-Draft-orange.svg)
![Last Updated: 23 March 2025](https://img.shields.io/badge/Last_Updated-23_March_2025-teal.svg)
![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)
![Maintainer: Center for Cyber Intelligence](https://img.shields.io/badge/Maintainer-Center_for_Cyber_Intelligence-darkblue.svg)
![Views](https://img.shields.io/github/watchers/centerforcyberintelligence/CTI-AIU?label=Views&style=social)

# Appendix D: Selected Resources

This appendix provides a curated collection of resources for implementing the CTI-AIU Control Framework. These resources include standards, frameworks, tools, research papers, and community initiatives relevant to the responsible use of GenAI in Cyber Threat Intelligence operations.

## Standards and Frameworks

### AI Governance and Risk Management

| Resource | Description | Link |
|----------|-------------|------|
| **NIST AI Risk Management Framework (AI RMF)** | A voluntary framework to better manage risks related to AI systems, focusing on governance, mapping, measurement, and management. | [NIST AI RMF](https://www.nist.gov/itl/ai-risk-management-framework) |
| **ISO/IEC 42001:2023** | International standard for Artificial Intelligence Management System (AIMS) requirements. | [ISO/IEC 42001:2023](https://www.iso.org/standard/81230.html) |
| **ISO/IEC 23894:2023** | Risk management guidelines for AI systems. | [ISO/IEC 23894:2023](https://www.iso.org/standard/77304.html) |
| **OECD AI Principles** | International principles promoting innovative and trustworthy AI that respects human rights and democratic values. | [OECD AI Principles](https://www.oecd.org/digital/artificial-intelligence/) |
| **European AI Act (Proposed)** | Regulatory framework for AI in the European Union, establishing obligations for providers and users based on risk categorization. | [EU AI Act](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai) |

### Cybersecurity and CTI Standards

| Resource | Description | Link |
|----------|-------------|------|
| **NIST SP 800-53 Rev. 5** | Security and privacy controls for information systems and organizations, including relevant controls for AI integration. | [NIST SP 800-53](https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final) |
| **MITRE ATLAS** | Adversarial Threat Landscape for Artificial-Intelligence Systems framework, mapping attack techniques against ML systems. | [MITRE ATLAS](https://atlas.mitre.org/) |
| **CISA Secure AI Development Guidelines** | Guidelines for secure development, deployment, and use of AI systems in federal agencies. | [CISA Guidelines](https://www.cisa.gov/topics/technology-security/artificial-intelligence-security) |
| **ISO/IEC 27001:2022** | Information security management system requirements, with applicable controls for AI systems in security contexts. | [ISO/IEC 27001](https://www.iso.org/standard/27001) |
| **ENISA AI Cybersecurity Guidelines** | Guidelines from the EU Agency for Cybersecurity on securing AI systems. | [ENISA AI Guidelines](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges) |

## Tools and Implementations

| Resource | Description | Link |
|----------|-------------|------|
| **AI Incident Database** | Collection of AI incidents and failures that can inform risk assessment and red teaming. | [Responsible AI Collaborative](https://incidentdatabase.ai/) |
| **OWASP ML Security Verification Standard** | A framework for testing the security of machine learning applications. | [OWASP MLSVS](https://owasp.org/www-project-machine-learning-security-verification-standard/) |
| **LangKit** | Open-source toolkit for evaluating language model outputs, including hallucination detection and factuality scoring. | [GitHub - LangKit](https://github.com/whylabs/langkit) |
| **AI Explainability 360** | Toolkit for explaining and interpreting AI model predictions. | [IBM AI Explainability 360](https://aix360.mybluemix.net/) |
| **NeMo Guardrails** | An open-source toolkit for implementing guardrails in LLM applications to ensure they are safe, secure, and responsible. | [NVIDIA NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails) |

## Research and Publications

| Resource | Description | Link |
|----------|-------------|------|
| **AI and Shared Security: Governance Approaches for the Second Quantum Revolution** | Research examining AI governance approaches across government, industry, and civil society. | [CSET Report](https://cset.georgetown.edu/publication/ai-and-shared-security/) |
| **Deepfake Threat Intelligence Framework** | Framework for analyzing and mitigating synthetic media threats. | [Atlantic Council Report](https://www.atlanticcouncil.org/in-depth-research-reports/report/deepfake-threat-intelligence-framework/) |
| **The Attacks of Artificial Intelligence: Anti-detection and Offensive Considerations** | Analysis of AI technologies being weaponized in cyber operations. | [BlackHat Whitepaper](https://i.blackhat.com/us-18/Thu-August-9/us-18-Finkelstein-The-Attacks-Of-Artificial-Intelligence-Anti-Detection-And-Offensive-Considerations-wp.pdf) |
| **Securing the Future of Artificial Intelligence and Machine Learning at Microsoft** | Microsoft's approach to AI security and governance. | [Microsoft Security](https://www.microsoft.com/security/blog/wp-content/uploads/2019/09/Securing-the-Future-of-AI-and-ML-at-Microsoft.pdf) |
| **State of AI Report** | Annual report on developments in AI research, industry, politics, and safety. | [State of AI Report](https://www.stateof.ai/) |

## Community Resources

| Resource | Description | Link |
|----------|-------------|------|
| **Partnership on AI** | Multi-stakeholder organization developing best practices for responsible AI. | [Partnership on AI](https://partnershiponai.org/) |
| **IEEE Ethics in Action** | Initiative focused on ethical considerations in autonomous and intelligent systems. | [IEEE Ethics in Action](https://ethicsinaction.ieee.org/) |
| **AI Security Alliance** | Community focused on securing AI across the development lifecycle. | [AI Security Alliance](https://www.aisecurityalliance.org/) |
| **FIRST CTI SIG** | CTI Special Interest Group within the Forum of Incident Response and Security Teams. | [FIRST CTI SIG](https://www.first.org/global/sigs/cti/) |
| **WEF Responsible Use of Technology** | World Economic Forum community focused on responsible technology use. | [WEF Community](https://www.weforum.org/communities/gfc-on-ai-for-humanity) |

## Learning Resources

| Resource | Description | Link |
|----------|-------------|------|
| **Machine Learning Security Evasion Competition (MLSEC)** | Competition challenging participants to evade ML-based malware detectors. | [MLSEC](https://mlsec.io/) |
| **Adversarial ML Threat Matrix** | Framework for analyzing and categorizing adversarial attacks against ML systems. | [MITRE & Microsoft](https://github.com/mitre/advmlthreatmatrix) |
| **Prompt Engineering Guide** | Comprehensive guide to prompt engineering techniques and best practices. | [Prompt Engineering Guide](https://www.promptingguide.ai/) |
| **AI Risk Management Playbook** | Practical guidance for implementing the NIST AI RMF. | [NIST AI RMF Playbook](https://airc.nist.gov/AI-RMF-Quick-Start) |
| **System Cards** | Templates and guidelines for creating transparency documentation for AI systems. | [Partnership on AI](https://partnershiponai.org/paper/system-cards-a-practical-standard-for-responsible-ai-documentation/) |

---

*For suggestions on additional resources or to report outdated links, please submit a pull request following the guidelines in [CONTRIBUTE.md](../CONTRIBUTE.md).* 