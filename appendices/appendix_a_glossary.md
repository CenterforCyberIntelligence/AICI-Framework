# Appendix A: Glossary

<div align="center" style="display: flex; flex-wrap: wrap; justify-content: center; gap: 10px; margin-bottom: 20px;">
  <img src="https://img.shields.io/badge/Version-1.0.0-blue.svg" alt="Version: 1.0.0" />
  <img src="https://img.shields.io/badge/Status-Final-purple.svg" alt="Status: Final" />
  <img src="https://img.shields.io/badge/Last_Updated-23_March_2025-teal.svg" alt="Last Updated: 23 March 2025" />
  <img src="https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg" alt="License: CC BY-NC-ND 4.0" />
  <img src="https://img.shields.io/badge/Maintainer-Center_for_Cyber_Intelligence-darkblue.svg" alt="Maintainer: Center for Cyber Intelligence" />
  <img src="https://hits.sh/github.com/centerforcyberintelligence/CTI-AIU.svg?label=Views&color=6e5494" alt="Views" />
</div>


This glossary provides definitions for key terms used throughout the CTI-AIU Control Framework. These definitions are intended to ensure consistent understanding of terminology specific to Generative AI in Cyber Threat Intelligence contexts.

## Core Terminology

### Generative AI Terms

| Term | Definition |
|------|------------|
| **GenAI** | Generative Artificial Intelligence – AI systems capable of producing text, images, code, or other data outputs based on patterns learned from training data. |
| **Foundation Model** | Large-scale AI models trained on broad data sets that serve as a base for fine-tuning for specific applications. |
| **Hallucination** | The phenomenon where a GenAI model generates plausible but incorrect or fabricated information, presenting it with apparent confidence. |
| **Prompt Engineering** | The practice of designing and refining input text (prompts) to elicit desired outputs from GenAI systems. |
| **Prompt Injection** | A technique where malicious instructions are embedded within seemingly benign prompts to manipulate AI system outputs. |
| **Fine-tuning** | The process of further training a pre-trained model on a specific dataset to adapt it for specialized tasks. |
| **Model Drift** | The degradation of model performance over time as real-world data begins to differ from the data on which the model was trained. |
| **Token** | The basic unit of text processing in language models, which can be a word, part of a word, or a character depending on the tokenization method. |

### Cyber Threat Intelligence Terms

| Term | Definition |
|------|------------|
| **CTI** | Cyber Threat Intelligence – The practice of analyzing adversary behavior, capabilities, and infrastructure to support defensive cybersecurity operations and strategic decision-making. |
| **PIR** | Priority Intelligence Requirement – A specific information need that drives intelligence collection and analysis priorities. |
| **TTP** | Tactics, Techniques, and Procedures – The patterns of activities and methods associated with specific threat actors or groups. |
| **IOC** | Indicator of Compromise – Technical artifacts that identify potentially malicious activity on a system or network. |
| **Attribution** | The process of identifying the actor responsible for a specific cyber attack or campaign. |
| **OSINT** | Open Source Intelligence – Intelligence collected from publicly available sources. |
| **Intelligence Lifecycle** | The cyclical process of direction, collection, processing, analysis, dissemination, and feedback in intelligence operations. |

### Control Framework Terms

| Term | Definition |
|------|------------|
| **Control** | A safeguard or countermeasure prescribed to meet a control objective and to reduce risk. |
| **Control Family** | A logical grouping of controls designed to address related risks or functions. |
| **Control Enhancement** | Statements that provide additional information or guidance for implementing a control. |
| **Control Objective** | The intended outcome of implementing a specific control. |
| **Assessment Criteria** | Specific, measurable methods to evaluate whether a control has been implemented properly. |
| **Maturity Level** | A measure of the completeness and effectiveness of a process or practice. |

## Extended Terminology

| Term | Definition |
|------|------------|
| **Adversarial Testing** | Deliberate attempts to make GenAI systems produce harmful, biased, or inaccurate outputs to identify vulnerabilities. |
| **Red Teaming** | The practice of challenging systems, plans, or assumptions by assuming an adversarial role or perspective. |
| **Data Provenance** | The documented history of data, including its origin, chain of custody, and transformations. |
| **Human-in-the-Loop (HITL)** | Systems or processes that require human interaction, validation, or decision-making at critical points. |
| **AI Ethics** | The field concerned with ensuring AI systems are designed and deployed in ways that align with human values and societal well-being. |
| **Explainability** | The degree to which the outputs or behaviors of an AI system can be understood by humans. |
| **Bias** | Systematic errors in AI outputs that result from imbalances or prejudices in training data or algorithms. |

---

*For updates to this glossary or to suggest additional terms, please submit a pull request following the guidelines in [CONTRIBUTE.md](../CONTRIBUTE.md).* 