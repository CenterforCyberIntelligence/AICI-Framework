![Version: 1.0.0](https://img.shields.io/badge/Version-1.0.0-blue.svg)
![Status: Draft](https://img.shields.io/badge/Status-Draft-orange.svg)
![Last Updated: 23 March 2025](https://img.shields.io/badge/Last_Updated-23_March_2025-teal.svg)
![License: CC BY-NC-ND 4.0](https://img.shields.io/badge/License-CC_BY--NC--ND_4.0-lightgrey.svg)
![Maintainer: Center for Cyber Intelligence](https://img.shields.io/badge/Maintainer-Center_for_Cyber_Intelligence-darkblue.svg)
![Views](https://img.shields.io/github/watchers/centerforcyberintelligence/CTI-AIU?label=Views&style=social)

# CTI-AI-7
## **Transparency and Community Accountability**

### **Control Statement**
Organizations shall maintain transparency regarding the use of Generative AI (GenAI) in cyber threat intelligence (CTI) operations and actively contribute to community knowledge-sharing and accountability initiatives. This includes publishing anonymized usage insights, disclosing evaluation results where appropriate, and collaborating to develop industry-wide safeguards for GenAI in CTI contexts.

---
### **Control Objectives**
- Promote responsible innovation through open, transparent GenAI practices within the CTI community.
- Foster collective learning, risk mitigation, and operational maturity across organizations using GenAI.
- Enable peer benchmarking and cross-sector analysis of GenAI model behavior, vulnerabilities, and impact.
- Reduce secrecy-driven deployment risks by encouraging mutual accountability and real-world feedback loops.

---

### **Control Requirements**

Organizations must:

1. **Maintain a Public Use Policy**  
    Publish a high-level summary of how GenAI is integrated into CTI workflows, including intended use cases, general oversight methods, and guiding ethical principles.
2. **Disclose Lessons Learned and Case Studies**  
    Share anonymized summaries of internal testing outcomes, operational successes and failures, and implementation challenges through community channels, white papers, or shared repositories.
3. **Participate in Community Governance**  
    Contribute to working groups, public consultations, or open standard initiatives that seek to regulate or advance the ethical use of GenAI in threat intelligence.
4. **Track and Share Model Behavior Insights**  
    Aggregate and publish non-sensitive data on model behavior—such as hallucination frequency, prompt types used, or risk trends—while preserving confidentiality and security constraints.
5. **Support Open Benchmarking**  
    When feasible, submit test results of GenAI systems to trusted public benchmarking frameworks or repositories used to evaluate GenAI in CTI or adjacent fields.
6. **Enable External Review or Audit**  
    Provide avenues for external review of GenAI usage policies and practices, either through third-party audits, academic partnerships, or mutual assurance agreements.
7. **Acknowledge GenAI-Generated Content**  
    Clearly label and distinguish GenAI-generated components of CTI outputs, particularly in shared environments, cross-organizational reports, or publicly disseminated briefings.

---
### **Assessment Criteria**
To assess conformance with this control:
- Confirm publication of an organizational GenAI use policy for CTI.
- Verify contributions to open-source tools, shared datasets, or benchmark repositories.
- Review records of community participation, working group involvement, or cross-sector collaboration.
- Inspect documentation of shared case studies, reports, or anonymized incident learnings.
- Validate the existence of GenAI output labeling mechanisms and public transparency disclosures.

---
### **Implementation Considerations**
- Smaller organizations can fulfill transparency obligations by contributing to community wikis, shared mailing lists, or moderated forums without revealing sensitive implementation details.
- Consider leveraging collaboration platforms (e.g., ISACs, SIGs, CCI forums) to facilitate secure and trusted knowledge sharing.
- GenAI output labeling should follow established practices for AI content disclosure, such as embedded footnotes, metadata tags, or visual indicators.
- Transparency must be balanced with operational security, protecting methods, sources, and proprietary data.

---

### **Cross-References**

- NIST SP 800-53 Rev. 5:
  - AC-21 (Information Sharing)
  - PM-15 (Security and Privacy Groups and Associations)
  - PM-28 (Risk Framing)
  - SA-9 (External System Services)
  - PM-25 (Minimization of Personally-Identifiable Information)
  - PM-16 (Threat Awareness Program)
  - PM-24 (Data Transparency)
  - IR-4 (Incident Handling)

- NIST AI RMF:
  - Govern 1.3 (Accountability)
  - Govern 2.1 (Public Transparency)
  - Govern 2.3 (Shared Responsibility)
  - Govern 3.1 (Community Engagement)
  - Govern 3.3 (External Evaluation)
  - Manage 2.1 (Explainability)
  - Manage 2.2 (AI Transparency)
  - Manage 3.1 (Knowledge Management)

- ISO/IEC 42001:2023:
  - 7.4 (Communication)
  - 9.1.3 (Analysis and evaluation)
  - 10.1 (Continual improvement)
  - 10.2 (Nonconformity and corrective action)
  - 10.3 (Continual improvement of AI management system)

- ISO/IEC 38507:2022:
  - Governance implications of AI for organizations
  - Transparency and explainability requirements
  - Stakeholder engagement for AI governance

- OECD Principles on AI Transparency:
  - Open publication of research findings
  - Cross-sectoral information sharing
  - Transparency in algorithmic decision making
  - Public engagement and community standards

- MITRE ATLAS:
  - Collaborative Defense Methodologies
  - Threat Information Sharing
  - Community-based Detection Strategies

- European AI Act (Proposed):
  - Article 13 (Transparency and provision of information to users)
  - Article 50 (AI Regulatory Sandboxes)
  - Article 53 (Measures for small-scale providers and users)
  - Article 14 (Human oversight)
  - Article 60 (EU database for stand-alone high-risk AI systems)

- ENISA Guidelines:
  - Transparency principle (Explainability and Documentation)
  - Accountability principle (Public Disclosure)
  - Security principle (Information Sharing)

- WEF Responsible Use of Technology:
  - Shared learnings on AI safety
  - Collaborative improvement frameworks
  - Open peer review processes

### **Related Controls**
- [CTI-AI-1](./CTI-AI-1.md): Use of Generative AI in CTI
- [CTI-AI-2](./CTI-AI-2.md): Ethical Use Enforcement
- [CTI-AI-3](./CTI-AI-3.md): Human Oversight Requirement
- [CTI-AI-6](./CTI-AI-6.md): Red Teaming and Model Evaluation

### **Additional Resources**
- For a complete overview of all controls, see [Whitepaper.md](./Whitepaper.md)
- For information on contributing enhancements to this control, see [CONTRIBUTE.md](./CONTRIBUTE.md)